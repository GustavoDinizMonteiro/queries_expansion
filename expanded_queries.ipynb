{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import nltk\n",
    "\n",
    "from typing import List, Dict\n",
    "from unidecode import unidecode\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gustavo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# loading dependencies\n",
    "data = pandas.read_csv(\"estadao_noticias_eleicao.csv\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# cleaning null data\n",
    "data = data.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joyning column of title and sub-title of artile with column of content.\n",
    "data['articles'] = data['titulo']  + ' ' + data['subTitulo'] + ' ' + data['conteudo']\n",
    "\n",
    "# lambda funcion to normalize text to lower case.\n",
    "normalize = lambda text: unidecode(text.lower())\n",
    "\n",
    "# lambda function to split text in tokens.\n",
    "tokenize = lambda row: row.split()\n",
    "\n",
    "# normalizing and tokenizing articles.\n",
    "data['articles'] = data['articles'].apply(normalize)\n",
    "data['tokens'] = data['articles'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(matrix_of_tokens: List[str], doc_ids: List[int]) -> Dict[str, List[int]]:\n",
    "    \"\"\"Create a inverted index with all tokens and yours document Ids.\n",
    "    :param matrix_of_tokens: matrix of article tokens lists.\n",
    "    :param doc_ids: list of document ids of all articles.\n",
    "    :returns: A inverted index with all tokens and yours document Ids.\n",
    "    \"\"\"\n",
    "    index = {}\n",
    "    for i in range(len(matrix_of_tokens)):\n",
    "        for token in set(matrix_of_tokens[i]):\n",
    "            if token in index.keys():\n",
    "                index[token].append(doc_ids[i])\n",
    "            else:\n",
    "                index[token] = [doc_ids[i]]\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the inverted index\n",
    "inverted_index = create_index(data['tokens'], data['idNoticia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    n = len(vocab)\n",
    "   \n",
    "    vocab_to_index = {word:i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    bi_grams = list(bigrams(corpus))\n",
    "\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "\n",
    "    I=list()\n",
    "    J=list()\n",
    "    V=list()\n",
    "    \n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "\n",
    "        I.append(vocab_to_index[previous])\n",
    "        J.append(vocab_to_index[current])\n",
    "        V.append(count)\n",
    "        \n",
    "    co_occurrence_matrix = sparse.coo_matrix((V,(I,J)), shape=(n,n))\n",
    "\n",
    "    return co_occurrence_matrix, vocab_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_ = stopwords.words('portuguese')\n",
    "filtered_tokens = data['tokens'].apply(lambda tokens: [token for token in tokens if token not in stopword_])\n",
    "                                     \n",
    "tokens = [token for tokens_list in filtered_tokens for token in tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, vocab = co_occurrence_matrix(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "consultable_matrix = matrix.tocsr()\n",
    "\n",
    "def consult_frequency(w1, w2):\n",
    "    return(consultable_matrix[vocab[w1],vocab[w2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = list(zip(matrix.row, matrix.col, matrix.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_vocab = [k for k, v in vocab.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_words(word: str) -> List[str]:\n",
    "    \"\"\"Returns the top three words with co occurrence with the passed word.\n",
    "    :param word: passed word for get co occurrence.\n",
    "    :returns: A array with top three words with co occurrence with the passed word.\n",
    "    \"\"\"\n",
    "    index = index_to_vocab.index(word)\n",
    "    filtered = list(filter((lambda x: x[0] == index or x[1] == index), tree))\n",
    "    top_tree = list(map((lambda x: index_to_vocab[x[0]] if x[0] != index else index_to_vocab[x[1]]), filtered[:3]))\n",
    "    return top_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_search(query: str) -> List[str]:\n",
    "    words = list(set(query.split(' ')))\n",
    "    related_words = list(map(lambda w: get_related_words(w), words))\n",
    "    related_words = [word for list_of_words in related_words for word in list_of_words]\n",
    "    words += related_words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str) -> List[int]:\n",
    "    words = expand_search(query)\n",
    "    ids = [inverted_index.get(word) for word in words]\n",
    "    ids = [id for list_of_ids in ids for id in list_of_ids]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respondendo as Perguntas\n",
    "----------------------------------------\n",
    "\n",
    "Para isso foram escolhidas 3 palavras:\n",
    "* Dilma\n",
    "* Lula\n",
    "* Temer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para dilma as 3 palavras relacionadas são: ['cabras', 'arroubos', 'sri']\n",
      "Para lula as 3 palavras relacionadas são: ['ultimos,', 'compartimentada', 'habilidade.\"']\n",
      "Para temer as 3 palavras relacionadas são: ['arbitrariedade,', 'estocolmo,', '\"exaustivamente']\n"
     ]
    }
   ],
   "source": [
    "dilma = get_related_words('dilma')\n",
    "lula = get_related_words('lula')\n",
    "temer = get_related_words('temer')\n",
    "\n",
    "print(\"Para dilma as 3 palavras relacionadas são:\", dilma)\n",
    "print(\"Para lula as 3 palavras relacionadas são:\", lula)\n",
    "print(\"Para temer as 3 palavras relacionadas são:\", temer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Os resultados parecem fazer bastante sentido, pois para os dois primeiros nomes\n",
    "    os resultados são basicamente os sobrenomes dos mesmos, para os dois primeiros casos\n",
    "    também se vê o problema de repetição te termos por não ter sido filtrado os termos \n",
    "    com vírgulas no final. Para a terceira palavra escolhida também uma das três palavras\n",
    "    foi um dos nomes do mesmo, e outra das palavras é nome do partido a qual pertence \n",
    "    michel temer, apenas uma das palavras não dá pra se dizer com certeza se ela está \n",
    "    relacionada.\n",
    "        No geral a expanssão das consultas parece estar funcionando bem, trazendo palavras \n",
    "    que parecem estar bastante relacionadas com as passadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Na minha opinião a expanssão das consultas é mais adequada pra melhorar o recall,\n",
    "    pois o recall é a fração dos documentos relevantes que são recuperados com êxito\n",
    "    então quanto mais docs são recuperados, mais chances de ter documento relevantes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
